#!/usr/bin/env python3
"""
TimeWalker Supabase ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¡œì»¬ JSON íŒŒì¼ì„ ì½ì–´ Supabase ë°ì´í„°ë² ì´ìŠ¤ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python migrate_data.py --url <SUPABASE_URL> --key <SERVICE_ROLE_KEY>

í•„ìˆ˜ íŒ¨í‚¤ì§€:
    pip install supabase python-dotenv

í™˜ê²½ë³€ìˆ˜ (.env íŒŒì¼):
    SUPABASE_URL=https://xxx.supabase.co
    SUPABASE_SERVICE_ROLE_KEY=eyJxxx...
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Any

try:
    from supabase import create_client, Client
    from dotenv import load_dotenv
except ImportError:
    print("í•„ìˆ˜ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install supabase python-dotenv")
    sys.exit(1)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
PROJECT_ROOT = Path(__file__).parent.parent.parent
ASSETS_DATA_DIR = PROJECT_ROOT / "assets" / "data"

# ë°ì´í„°ì…‹ ì •ì˜ (JSON íŒŒì¼ -> í…Œì´ë¸” ë§¤í•‘)
DATASETS = {
    "characters": {
        "file": "characters.json",
        "table": "characters",
        "staging_table": "stg_characters",
    },
    "dialogues": {
        "file": "dialogues.json",
        "table": "dialogues",
        "staging_table": "stg_dialogues",
    },
    "locations": {
        "file": "locations.json",
        "table": "locations",
        "staging_table": "stg_locations",
    },
    "encyclopedia_entries": {
        "file": "encyclopedia.json",
        "table": "encyclopedia_entries",
        "staging_table": "stg_encyclopedia_entries",
    },
    "quizzes": {
        "file": "quizzes.json",
        "table": "quizzes",
        "staging_table": "stg_quizzes",
    },
}


def load_json(file_path: Path) -> list[dict[str, Any]]:
    """JSON íŒŒì¼ ë¡œë“œ"""
    with open(file_path, "r", encoding="utf-8") as f:
        return json.load(f)


def clear_staging_table(client: Client, table_name: str) -> None:
    """Staging í…Œì´ë¸” ë¹„ìš°ê¸°"""
    try:
        # Delete all rows (Supabase doesn't have TRUNCATE via API)
        client.table(table_name).delete().neq("payload", {}).execute()
        print(f"  âœ“ {table_name} ë¹„ìš°ê¸° ì™„ë£Œ")
    except Exception as e:
        print(f"  âš  {table_name} ë¹„ìš°ê¸° ì‹¤íŒ¨ (í…Œì´ë¸”ì´ ì—†ì„ ìˆ˜ ìˆìŒ): {e}")


def insert_to_staging(client: Client, table_name: str, data: list[dict]) -> int:
    """Staging í…Œì´ë¸”ì— ë°ì´í„° ì‚½ì…"""
    if not data:
        return 0

    # JSONB payload í˜•íƒœë¡œ ë³€í™˜
    payloads = [{"payload": item} for item in data]

    # ë°°ì¹˜ ì‚½ì… (ìµœëŒ€ 1000ê°œì”©)
    batch_size = 500
    inserted = 0

    for i in range(0, len(payloads), batch_size):
        batch = payloads[i : i + batch_size]
        try:
            client.table(table_name).insert(batch).execute()
            inserted += len(batch)
            print(f"  âœ“ {table_name}: {inserted}/{len(payloads)} ì‚½ì…ë¨")
        except Exception as e:
            print(f"  âœ— {table_name} ì‚½ì… ì˜¤ë¥˜: {e}")
            raise

    return inserted


def run_load_sql(client: Client) -> None:
    """load.sql ì‹¤í–‰ (staging -> main í…Œì´ë¸”)
    
    Note: Supabase Python í´ë¼ì´ì–¸íŠ¸ëŠ” ì§ì ‘ SQL ì‹¤í–‰ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    load.sqlì€ Supabase Dashboardì˜ SQL Editorì—ì„œ ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    print("\nâš ï¸  load.sqlì€ Supabase Dashboardì—ì„œ ìˆ˜ë™ ì‹¤í–‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    print("   1. Supabase Dashboard -> SQL Editor ì´ë™")
    print("   2. tools/supabase/load.sql ë‚´ìš© ë³µì‚¬ & ì‹¤í–‰")


def update_content_versions(client: Client, datasets: list[str], version: str = "v1") -> None:
    """content_versions í…Œì´ë¸” ì—…ë°ì´íŠ¸"""
    for dataset in datasets:
        try:
            client.table("content_versions").upsert({
                "dataset": dataset,
                "version": version,
                "checksum": None,
            }).execute()
            print(f"  âœ“ content_versions: {dataset} -> {version}")
        except Exception as e:
            print(f"  âš  content_versions ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def migrate_dataset(client: Client, dataset_name: str, config: dict) -> dict:
    """ë‹¨ì¼ ë°ì´í„°ì…‹ ë§ˆì´ê·¸ë ˆì´ì…˜"""
    file_path = ASSETS_DATA_DIR / config["file"]
    staging_table = config["staging_table"]

    print(f"\nğŸ“¦ {dataset_name} ë§ˆì´ê·¸ë ˆì´ì…˜...")
    print(f"   íŒŒì¼: {file_path}")

    if not file_path.exists():
        print(f"   âœ— íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
        return {"status": "error", "count": 0}

    # JSON ë¡œë“œ
    data = load_json(file_path)
    print(f"   ë¡œë“œëœ í•­ëª©: {len(data)}ê°œ")

    # Staging í…Œì´ë¸” ë¹„ìš°ê¸°
    clear_staging_table(client, staging_table)

    # Staging í…Œì´ë¸”ì— ì‚½ì…
    inserted = insert_to_staging(client, staging_table, data)

    return {"status": "success", "count": inserted}


def main():
    parser = argparse.ArgumentParser(description="TimeWalker Supabase ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument("--url", help="Supabase URL")
    parser.add_argument("--key", help="Supabase Service Role Key")
    parser.add_argument(
        "--datasets",
        nargs="+",
        choices=list(DATASETS.keys()) + ["all"],
        default=["all"],
        help="ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë°ì´í„°ì…‹ (ê¸°ë³¸: all)",
    )
    parser.add_argument("--dry-run", action="store_true", help="ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜")
    args = parser.parse_args()

    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv(PROJECT_ROOT / ".env")

    supabase_url = args.url or os.getenv("SUPABASE_URL")
    supabase_key = args.key or os.getenv("SUPABASE_SERVICE_ROLE_KEY")

    if not supabase_url or not supabase_key:
        print("âŒ Supabase URLê³¼ Service Role Keyê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   --url, --key ì˜µì…˜ ë˜ëŠ” .env íŒŒì¼ì„ ì„¤ì •í•˜ì„¸ìš”.")
        sys.exit(1)

    print("=" * 60)
    print("ğŸš€ TimeWalker Supabase ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜")
    print("=" * 60)
    print(f"URL: {supabase_url}")
    print(f"Data Dir: {ASSETS_DATA_DIR}")

    if args.dry_run:
        print("\nâš ï¸  DRY RUN ëª¨ë“œ - ì‹¤ì œ ì—…ë¡œë“œ ì—†ìŒ")
        for name, config in DATASETS.items():
            file_path = ASSETS_DATA_DIR / config["file"]
            if file_path.exists():
                data = load_json(file_path)
                print(f"  {name}: {len(data)}ê°œ í•­ëª©")
            else:
                print(f"  {name}: íŒŒì¼ ì—†ìŒ")
        return

    # Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client: Client = create_client(supabase_url, supabase_key)

    # ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë°ì´í„°ì…‹ ê²°ì •
    target_datasets = list(DATASETS.keys()) if "all" in args.datasets else args.datasets

    results = {}
    for name in target_datasets:
        config = DATASETS[name]
        results[name] = migrate_dataset(client, name, config)

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼")
    print("=" * 60)
    
    success_count = 0
    for name, result in results.items():
        status_icon = "âœ“" if result["status"] == "success" else "âœ—"
        print(f"  {status_icon} {name}: {result['count']}ê°œ")
        if result["status"] == "success":
            success_count += 1

    print(f"\nì´ {success_count}/{len(results)} ë°ì´í„°ì…‹ ì„±ê³µ")

    # load.sql ì‹¤í–‰ ì•ˆë‚´
    run_load_sql(client)

    # content_versions ì—…ë°ì´íŠ¸
    print("\nğŸ“ content_versions ì—…ë°ì´íŠ¸...")
    successful_datasets = [name for name, result in results.items() if result["status"] == "success"]
    update_content_versions(client, successful_datasets)

    print("\nâœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("  1. Supabase Dashboard -> SQL Editor ì´ë™")
    print("  2. tools/supabase/load.sql ë‚´ìš© ì‹¤í–‰")
    print("  3. ì•±ì—ì„œ Supabase ì—°ë™ í…ŒìŠ¤íŠ¸")


if __name__ == "__main__":
    main()
